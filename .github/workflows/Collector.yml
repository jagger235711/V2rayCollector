name: Collector
on:
  workflow_dispatch:
  schedule:
    - cron: "0 */8 * * *"

env:
  # ËæìÂÖ•Êñá‰ª∂ÈÖçÁΩÆ
  RESOURCE_DIR: $GITHUB_WORKSPACE/resource
  # INPUT_SUBLIST: $GITHUB_WORKSPACE/resource/subList.csv
  # INPUT_CHANNELS: $GITHUB_WORKSPACE/resource/channels.csv
  
  # ËæìÂá∫ÁõÆÂΩïÈÖçÁΩÆ
  RESOULTS_DIR: $GITHUB_WORKSPACE/results
  # OUTPUT_MIXED: $GITHUB_WORKSPACE/results/mixed.txt
  # OUTPUT_TESTED: $GITHUB_WORKSPACE/results/mixed_tested.json
  
  # Â∑•ÂÖ∑Ë∑ØÂæÑÈÖçÁΩÆ

  TOOLS_DIR: $GITHUB_WORKSPACE/tool
  # SPEEDTEST_TOOL: $GITHUB_WORKSPACE/tool/speedTest_singtools/singtools
  # SPEEDTEST_CONFIG: $GITHUB_WORKSPACE/tool/speedTest_singtools/config.json

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Delete old Releases and Workflows
        uses: ophub/delete-releases-workflows@main
        with:
          gh_token: ${{secrets.GITHUB_TOKEN}}
          delete_releases: true
          releases_keep_latest: 3
          delete_tags: true
          delete_workflows: true
          workflows_keep_day: 3

      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Cleanup results directory
        run: |
          rm -rf ${{ env.RESOULTS_DIR }}/*
          mkdir -p ${{ env.RESOULTS_DIR }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt --cache-dir ~/.cache/pip

      - name: Deduplicate URLs
        run: |
          python deduplicate.py ${{ env.RESOURCE_DIR }}/subList.csv ${{ env.RESOURCE_DIR }}/subList.csv
          echo "ÂéªÈáçÂÆåÊàêÔºåÂ§ÑÁêÜ‰∫Ü $(wc -l < ${{ env.RESOURCE_DIR }}/subList.csv) ‰∏™URL"
          python deduplicate.py ${{ env.RESOURCE_DIR }}/channels.csv ${{ env.RESOURCE_DIR }}/channels.csv
          echo "ÂéªÈáçÂÆåÊàêÔºåÂ§ÑÁêÜ‰∫Ü $(wc -l < ${{ env.RESOURCE_DIR }}/channels.csv) ‰∏™URL"
          echo "Â§á‰ªΩÊñá‰ª∂:"
          ls -la ${{ env.RESOURCE_DIR }}/*.bak

      - name: Run Scrapy spider
        run: |
          scrapy crawl telegram_crawler

      - name: Cache Docker images
        uses: actions/cache@v3
        with:
          path: ~/.docker/cache
          key: ${{ runner.os }}-docker-${{ hashFiles('**/Dockerfile') }}
          restore-keys: |
            ${{ runner.os }}-docker-

      - name: Start subconverter and process URLs (Âπ∂ÂèëÁâàÊú¨)
        run: |
          set -e

          echo "=== üê≥ ÂÆπÂô®ÊåÇËΩΩË∞ÉËØïÂºÄÂßã ==="
          CONTAINER_NAME="subconverter_container"
          CONTAINER_PORT=25500
          HOST_PORT=25500
          IMAGE_NAME="asdlokj1qpi23/subconverter:latest"
          RESULT_DIR="${{ env.RESOULTS_DIR }}"
          RESOURCE_FILE="${{ env.RESOURCE_DIR }}/subList.csv"
          RESULT_FILE="${RESULT_DIR}/mixed_base64.txt"
          MAX_PER_BATCH=5
          MAX_CONCURRENT=4  # Âπ∂ÂèëÂ§ÑÁêÜÁöÑÊúÄÂ§ßÊâπÊ¨°Êï∞

          docker run -d --name $CONTAINER_NAME --restart=always \
            -v "$RESULT_DIR:/subconverter/results" \
            -p ${HOST_PORT}:${CONTAINER_PORT} \
            $IMAGE_NAME

          echo "Á≠âÂæÖ subconverter ÂêØÂä®..."
          for i in {1..30}; do
            if curl -s http://localhost:${HOST_PORT} >/dev/null; then
              echo "‚úÖ ÊúçÂä°Â∑≤Â∞±Áª™"
              break
            fi
            sleep 1
            if [ $i -eq 30 ]; then
              echo "‚ùå ÂêØÂä®Â§±Ë¥•"
              exit 1
            fi
          done

          if [ ! -f "$RESOURCE_FILE" ]; then
            echo "‚ùå ËµÑÊ∫êÊñá‰ª∂‰∏çÂ≠òÂú®: $RESOURCE_FILE"
            exit 1
          fi

          echo "ËØªÂèñËµÑÊ∫êÂàóË°®..."
          mapfile -t links < "$RESOURCE_FILE"
          total=${#links[@]}
          # echo "" > "$RESULT_FILE"
          TMP_BATCH_DIR=$(mktemp -d)

          process_batch() {
            local index=$1
            shift
            local batch=("$@")
            local batch_url
            batch_url=$(IFS='|'; echo "${batch[*]}")
            local request_url="http://localhost:${HOST_PORT}/sub?target=mixed&url=$batch_url"
            local temp_file="${TMP_BATCH_DIR}/batch_${index}.txt"
            
            echo "üîß ÊâπÊ¨° $index ËØ∑Ê±Ç: $request_url"
            if curl -fsSL "$request_url" -o "$temp_file"; then
              echo "‚úÖ ÊâπÊ¨° $index ÂÆåÊàê"
            else
              echo "‚ùå ÊâπÊ¨° $index ËØ∑Ê±ÇÂ§±Ë¥•"
              rm -f "$temp_file"
            fi
          }

          echo "=== üöÄ ÂºÄÂßãÂπ∂ÂèëÂ§ÑÁêÜ ==="
          job_count=0
          batch_index=1

          for ((i = 1; i < total; i += MAX_PER_BATCH)); do
            batch=("${links[@]:i:MAX_PER_BATCH}")
            process_batch "$batch_index" "${batch[@]}" &
            job_count=$((job_count + 1))
            batch_index=$((batch_index + 1))

            if (( job_count >= MAX_CONCURRENT )); then
              wait
              job_count=0
            fi
          done

          wait  # Á≠âÂæÖÊâÄÊúâÂâ©‰ΩôÂêéÂè∞‰ªªÂä°ÂÆåÊàê

          echo "üß© ÂêàÂπ∂ÊâÄÊúâÊâπÊ¨°ÁªìÊûú"
          sort -V "${TMP_BATCH_DIR}"/batch_*.txt >> "$RESULT_FILE"
          rm -rf "$TMP_BATCH_DIR"

          echo "üéâ ÊâÄÊúâÂ§ÑÁêÜÂÆåÊàêÔºåÁªìÊûúÊñá‰ª∂: $RESULT_FILE"

      - name: Decode base64 and append to mixed.txt
        run: |
          echo "Â∑•‰ΩúË∑ØÂæÑ: $GITHUB_WORKSPACE"
          echo "ÂΩìÂâçË∑ØÂæÑ: $(pwd)"
          echo "----------------"
          ls -R $GITHUB_WORKSPACE
          echo "----------------"
          base64 -d ${{ env.RESOULTS_DIR }}/mixed_base64.txt >> ${{ env.RESOULTS_DIR }}/mixed.txt

      - name: Clean mixed.txt content
        run: |
          if [ ! -f "${{ env.RESOULTS_DIR }}/mixed.txt" ]; then exit 1; fi
          cat "${{ env.RESOULTS_DIR }}/mixed.txt" | sed 's/^[ \t]*//;s/[ \t]*$//' | sed '/^$/d' > "${{ env.RESOULTS_DIR }}/mixed.txt.tmp"
          mv "${{ env.RESOULTS_DIR }}/mixed.txt.tmp" "${{ env.RESOULTS_DIR }}/mixed.txt"
          echo "Ê∏ÖÁêÜÂêéÁªüËÆ°Ôºö$(wc -l ${{ env.RESOULTS_DIR }}/mixed.txt) Ë°å $(wc -c ${{ env.RESOULTS_DIR }}/mixed.txt) Â≠óËäÇ"

      - name: Speedtest
        run: |
          if [ ! -f "${{ env.RESOULTS_DIR }}/mixed.txt" ]; then exit 1; fi
          chmod +x ${{ env.TOOLS_DIR }}/speedTest_singtools/singtools
          ${{ env.TOOLS_DIR }}/speedTest_singtools/singtools test -i ${{ env.RESOULTS_DIR }}/mixed.txt -c ${{ env.TOOLS_DIR }}/speedTest_singtools/config.json -o ${{ env.RESOULTS_DIR }}/mixed_tested.json -e fatal -f ""
          if [ ! -f "${{ env.RESOULTS_DIR }}/mixed_tested.json" ]; then exit 1; fi

      - name: Convert to mixed with base64encode
        run: |
          if [ ! -f "${{ env.RESOULTS_DIR }}/mixed_tested.json" ]; then exit 1; fi
          full_url="http://localhost:25500/sub?target=mixed&url=/subconverter/results/mixed_tested.json"
          echo "Ê≠£Âú®ËØ∑Ê±Ç: $full_url"
          if ! curl -v -o ${{ env.RESOULTS_DIR }}/mixed_tested.txt "$full_url"; then
            echo "curl failed: $(docker exec subconverter_container ls -l /subconverter/results)"
            exit 1
          fi
          if [ ! -s "${{ env.RESOULTS_DIR }}/mixed_tested.txt" ]; then exit 1; fi

      - name: Commit Changes
        run: |
          git config --local user.email "actions@github.com"
          git config --local user.name "GitHub Actions"
          git pull origin main
          git add ${{ env.RESOULTS_DIR }}/*
          git add ${{ env.RESOURCE_DIR }}/subList.csv
          git add ${{ env.RESOURCE_DIR }}/channels.csv
          git commit -m "‚úîÔ∏è $(date '+%Y-%m-%d %H:%M:%S') Collected" || echo "No changes to commit"

      - name: Push Changes
        uses: ad-m/github-push-action@master
        with:
          branch: main
